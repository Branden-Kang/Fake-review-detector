{"cells":[{"metadata":{},"cell_type":"markdown","source":"Thanks to [Reference](https://towardsdatascience.com/lstm-text-classification-using-pytorch-2c6c657f8fc0)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":20,"outputs":[{"output_type":"stream","text":"/kaggle/input/real-and-fake-news-dataset/news.csv\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Import libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Basic Libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nimport torch\n\n# Torchtext\nfrom torchtext.data import Field, TabularDataset, BucketIterator, Iterator\n\n# Transformers\nimport torch.nn as nn\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n\n# Training the model\nimport torch.optim as optim\n\n# Evaluate the model\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nimport seaborn as sns","execution_count":21,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install transformers","execution_count":22,"outputs":[{"output_type":"stream","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (3.0.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from transformers) (20.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.0.10)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2020.4.4)\nRequirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers) (0.0.43)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from transformers) (1.18.5)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.45.0)\nRequirement already satisfied: sentencepiece!=0.1.92 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.1.91)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.23.0)\nRequirement already satisfied: tokenizers==0.8.1.rc1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.8.1rc1)\nRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->transformers) (2.4.7)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from packaging->transformers) (1.14.0)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (0.14.1)\nRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (7.1.1)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2020.6.20)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.24.3)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.9)\nRequirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.0.4)\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/real-and-fake-news-dataset/news.csv')\ndf.head()","execution_count":23,"outputs":[{"output_type":"execute_result","execution_count":23,"data":{"text/plain":"   Unnamed: 0                                              title  \\\n0        8476                       You Can Smell Hillary’s Fear   \n1       10294  Watch The Exact Moment Paul Ryan Committed Pol...   \n2        3608        Kerry to go to Paris in gesture of sympathy   \n3       10142  Bernie supporters on Twitter erupt in anger ag...   \n4         875   The Battle of New York: Why This Primary Matters   \n\n                                                text label  \n0  Daniel Greenfield, a Shillman Journalism Fello...  FAKE  \n1  Google Pinterest Digg Linkedin Reddit Stumbleu...  FAKE  \n2  U.S. Secretary of State John F. Kerry said Mon...  REAL  \n3  — Kaydee King (@KaydeeKing) November 9, 2016 T...  FAKE  \n4  It's primary day in New York and front-runners...  REAL  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>title</th>\n      <th>text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>8476</td>\n      <td>You Can Smell Hillary’s Fear</td>\n      <td>Daniel Greenfield, a Shillman Journalism Fello...</td>\n      <td>FAKE</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>10294</td>\n      <td>Watch The Exact Moment Paul Ryan Committed Pol...</td>\n      <td>Google Pinterest Digg Linkedin Reddit Stumbleu...</td>\n      <td>FAKE</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3608</td>\n      <td>Kerry to go to Paris in gesture of sympathy</td>\n      <td>U.S. Secretary of State John F. Kerry said Mon...</td>\n      <td>REAL</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>10142</td>\n      <td>Bernie supporters on Twitter erupt in anger ag...</td>\n      <td>— Kaydee King (@KaydeeKing) November 9, 2016 T...</td>\n      <td>FAKE</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>875</td>\n      <td>The Battle of New York: Why This Primary Matters</td>\n      <td>It's primary day in New York and front-runners...</td>\n      <td>REAL</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Prepare columns\ndf['label'] = (df['label'] == 'FAKE').astype('int')\ndf['titletext'] = df['title'] + \". \" + df['text']\ndf = df.reindex(columns=['label', 'title', 'text', 'titletext'])","execution_count":24,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":25,"outputs":[{"output_type":"execute_result","execution_count":25,"data":{"text/plain":"   label                                              title  \\\n0      1                       You Can Smell Hillary’s Fear   \n1      1  Watch The Exact Moment Paul Ryan Committed Pol...   \n2      0        Kerry to go to Paris in gesture of sympathy   \n3      1  Bernie supporters on Twitter erupt in anger ag...   \n4      0   The Battle of New York: Why This Primary Matters   \n\n                                                text  \\\n0  Daniel Greenfield, a Shillman Journalism Fello...   \n1  Google Pinterest Digg Linkedin Reddit Stumbleu...   \n2  U.S. Secretary of State John F. Kerry said Mon...   \n3  — Kaydee King (@KaydeeKing) November 9, 2016 T...   \n4  It's primary day in New York and front-runners...   \n\n                                           titletext  \n0  You Can Smell Hillary’s Fear. Daniel Greenfiel...  \n1  Watch The Exact Moment Paul Ryan Committed Pol...  \n2  Kerry to go to Paris in gesture of sympathy. U...  \n3  Bernie supporters on Twitter erupt in anger ag...  \n4  The Battle of New York: Why This Primary Matte...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>title</th>\n      <th>text</th>\n      <th>titletext</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>You Can Smell Hillary’s Fear</td>\n      <td>Daniel Greenfield, a Shillman Journalism Fello...</td>\n      <td>You Can Smell Hillary’s Fear. Daniel Greenfiel...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>Watch The Exact Moment Paul Ryan Committed Pol...</td>\n      <td>Google Pinterest Digg Linkedin Reddit Stumbleu...</td>\n      <td>Watch The Exact Moment Paul Ryan Committed Pol...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>Kerry to go to Paris in gesture of sympathy</td>\n      <td>U.S. Secretary of State John F. Kerry said Mon...</td>\n      <td>Kerry to go to Paris in gesture of sympathy. U...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>Bernie supporters on Twitter erupt in anger ag...</td>\n      <td>— Kaydee King (@KaydeeKing) November 9, 2016 T...</td>\n      <td>Bernie supporters on Twitter erupt in anger ag...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>The Battle of New York: Why This Primary Matters</td>\n      <td>It's primary day in New York and front-runners...</td>\n      <td>The Battle of New York: Why This Primary Matte...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop rows with empty text\ndf.drop(df[df.text.str.len() < 5].index, inplace=True)","execution_count":26,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def trim_string(x):\n    x = x.split(maxsplit = 200)\n    x = ' '.join(x[:200])\n    return x","execution_count":27,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Trim text and titletext to 200\ndf['text'] = df['text'].apply(trim_string)\ndf['titletext'] = df['titletext'].apply(trim_string) ","execution_count":28,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split related to label\ndf_real = df[df['label'] == 0]\ndf_fake = df[df['label'] == 1]","execution_count":29,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train-test split\ndf_real_full_train, df_real_test = train_test_split(df_real, train_size = 0.1, random_state = 1)\ndf_fake_full_train, df_fake_test = train_test_split(df_fake, train_size = 0.1, random_state = 1)","execution_count":30,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train-valid split\ndf_real_train, df_real_valid = train_test_split(df_real_full_train, train_size = 0.8, random_state = 1)\ndf_fake_train, df_fake_valid = train_test_split(df_fake_full_train, train_size = 0.8, random_state = 1)","execution_count":31,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Concatenate splits of different labels\ndf_train = pd.concat([df_real_train, df_fake_train], ignore_index=True, sort=False)\ndf_valid = pd.concat([df_real_valid, df_fake_valid], ignore_index=True, sort=False)\ndf_test = pd.concat([df_real_test, df_fake_test], ignore_index=True, sort=False)","execution_count":32,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Write preprocessed data\ndf_train.to_csv('train.csv', index=False)\ndf_valid.to_csv('valid.csv', index=False)\ndf_test.to_csv('test.csv', index=False)","execution_count":33,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nprint(device)","execution_count":34,"outputs":[{"output_type":"stream","text":"cuda:0\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pwd","execution_count":35,"outputs":[{"output_type":"stream","text":"/kaggle/working\r\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fields\n\nlabel_field = Field(sequential=False, use_vocab=False, batch_first=True, dtype=torch.float)\ntext_field = Field(tokenize='spacy', lower=True, include_lengths=True, batch_first=True)\nfields = [('label', label_field), ('title', text_field), ('text', text_field), ('titletext', text_field)]\n\n# TabularDataset\n\ntrain, valid, test = TabularDataset.splits(path='/kaggle/working', train='train.csv', validation='valid.csv', test='test.csv',\n                                           format='CSV', fields=fields, skip_header=True)\n\n# Iterators\n\ntrain_iter = BucketIterator(train, batch_size=32, sort_key=lambda x: len(x.text),\n                            device=device, sort=True, sort_within_batch=True)\nvalid_iter = BucketIterator(valid, batch_size=32, sort_key=lambda x: len(x.text),\n                            device=device, sort=True, sort_within_batch=True)\ntest_iter = BucketIterator(test, batch_size=32, sort_key=lambda x: len(x.text),\n                            device=device, sort=True, sort_within_batch=True)\n\n# Vocabulary\n\ntext_field.build_vocab(train, min_freq=3)","execution_count":null,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n/opt/conda/lib/python3.7/site-packages/torchtext/data/example.py:68: UserWarning: Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n  warnings.warn('Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.', UserWarning)\n/opt/conda/lib/python3.7/site-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n  warnings.warn('Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.', UserWarning)\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"class LSTM(nn.Module):\n\n    def __init__(self, dimension=128):\n        super(LSTM, self).__init__()\n\n        self.embedding = nn.Embedding(len(text_field.vocab), 300)\n        self.dimension = dimension\n        self.lstm = nn.LSTM(input_size=300,\n                            hidden_size=dimension,\n                            num_layers=1,\n                            batch_first=True,\n                            bidirectional=True)\n        self.drop = nn.Dropout(p=0.5)\n\n        self.fc = nn.Linear(2*dimension, 1)\n\n    def forward(self, text, text_len):\n\n        text_emb = self.embedding(text)\n\n        packed_input = pack_padded_sequence(text_emb, text_len, batch_first=True, enforce_sorted=False)\n        packed_output, _ = self.lstm(packed_input)\n        output, _ = pad_packed_sequence(packed_output, batch_first=True)\n\n        out_forward = output[range(len(output)), text_len - 1, :self.dimension]\n        out_reverse = output[:, 0, self.dimension:]\n        out_reduced = torch.cat((out_forward, out_reverse), 1)\n        text_fea = self.drop(out_reduced)\n\n        text_fea = self.fc(text_fea)\n        text_fea = torch.squeeze(text_fea, 1)\n        text_out = torch.sigmoid(text_fea)\n\n        return text_out","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Save and Load Functions\n\ndef save_checkpoint(save_path, model, optimizer, valid_loss):\n\n    if save_path == None:\n        return\n    \n    state_dict = {'model_state_dict': model.state_dict(),\n                  'optimizer_state_dict': optimizer.state_dict(),\n                  'valid_loss': valid_loss}\n    \n    torch.save(state_dict, save_path)\n    print(f'Model saved to ==> {save_path}')\n\n\ndef load_checkpoint(load_path, model, optimizer):\n\n    if load_path==None:\n        return\n    \n    state_dict = torch.load(load_path, map_location=device)\n    print(f'Model loaded from <== {load_path}')\n    \n    model.load_state_dict(state_dict['model_state_dict'])\n    optimizer.load_state_dict(state_dict['optimizer_state_dict'])\n    \n    return state_dict['valid_loss']\n\n\ndef save_metrics(save_path, train_loss_list, valid_loss_list, global_steps_list):\n\n    if save_path == None:\n        return\n    \n    state_dict = {'train_loss_list': train_loss_list,\n                  'valid_loss_list': valid_loss_list,\n                  'global_steps_list': global_steps_list}\n    \n    torch.save(state_dict, save_path)\n    print(f'Model saved to ==> {save_path}')\n\n\ndef load_metrics(load_path):\n\n    if load_path==None:\n        return\n    \n    state_dict = torch.load(load_path, map_location=device)\n    print(f'Model loaded from <== {load_path}')\n    \n    return state_dict['train_loss_list'], state_dict['valid_loss_list'], state_dict['global_steps_list']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training Function\n\ndef train(model,\n          optimizer,\n          criterion = nn.BCELoss(),\n          train_loader = train_iter,\n          valid_loader = valid_iter,\n          num_epochs = 5,\n          eval_every = len(train_iter) // 2,\n          file_path = '/kaggle/working',\n          best_valid_loss = float(\"Inf\")):\n    \n    # initialize running values\n    running_loss = 0.0\n    valid_running_loss = 0.0\n    global_step = 0\n    train_loss_list = []\n    valid_loss_list = []\n    global_steps_list = []\n\n    # training loop\n    model.train()\n    for epoch in range(num_epochs):\n        for (labels, (title, title_len), (text, text_len), (titletext, titletext_len)), _ in train_loader:           \n            labels = labels.to(device)\n            titletext = titletext.to(device)\n            titletext_len = titletext_len.to(device)\n            output = model(titletext, titletext_len)\n\n            loss = criterion(output, labels)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            # update running values\n            running_loss += loss.item()\n            global_step += 1\n\n            # evaluation step\n            if global_step % eval_every == 0:\n                model.eval()\n                with torch.no_grad():                    \n                  # validation loop\n                  for (labels, (title, title_len), (text, text_len), (titletext, titletext_len)), _ in valid_loader:\n                        labels = labels.to(device)\n                        titletext = titletext.to(device)\n                        titletext_len = titletext_len.to(device)\n                        output = model(titletext, titletext_len)\n\n                        loss = criterion(output, labels)\n                        valid_running_loss += loss.item()\n\n                # evaluation\n                average_train_loss = running_loss / eval_every\n                average_valid_loss = valid_running_loss / len(valid_loader)\n                train_loss_list.append(average_train_loss)\n                valid_loss_list.append(average_valid_loss)\n                global_steps_list.append(global_step)\n\n                # resetting running values\n                running_loss = 0.0                \n                valid_running_loss = 0.0\n                model.train()\n\n                # print progress\n                print('Epoch [{}/{}], Step [{}/{}], Train Loss: {:.4f}, Valid Loss: {:.4f}'\n                      .format(epoch+1, num_epochs, global_step, num_epochs*len(train_loader),\n                              average_train_loss, average_valid_loss))\n                \n                # checkpoint\n                if best_valid_loss > average_valid_loss:\n                    best_valid_loss = average_valid_loss\n                    save_checkpoint(file_path + '/model.pt', model, optimizer, best_valid_loss)\n                    save_metrics(file_path + '/metrics.pt', train_loss_list, valid_loss_list, global_steps_list)\n    \n    save_metrics(file_path + '/metrics.pt', train_loss_list, valid_loss_list, global_steps_list)\n    print('Finished Training!')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = LSTM().to(device)\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\ntrain(model=model, optimizer=optimizer, num_epochs=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_loss_list, valid_loss_list, global_steps_list = load_metrics('/kaggle/working' + '/metrics.pt')\nplt.plot(global_steps_list, train_loss_list, label='Train')\nplt.plot(global_steps_list, valid_loss_list, label='Valid')\nplt.xlabel('Global Steps')\nplt.ylabel('Loss')\nplt.legend()\nplt.show() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Evaluation Function\n\ndef evaluate(model, test_loader, version='title', threshold=0.5):\n    y_pred = []\n    y_true = []\n\n    model.eval()\n    with torch.no_grad():\n        for (labels, (title, title_len), (text, text_len), (titletext, titletext_len)), _ in test_loader:           \n            labels = labels.to(device)\n            titletext = titletext.to(device)\n            titletext_len = titletext_len.to(device)\n            output = model(titletext, titletext_len)\n\n            output = (output > threshold).int()\n            y_pred.extend(output.tolist())\n            y_true.extend(labels.tolist())\n    \n    print('Classification Report:')\n    print(classification_report(y_true, y_pred, labels=[1,0], digits=4))\n    \n    cm = confusion_matrix(y_true, y_pred, labels=[1,0])\n    ax= plt.subplot()\n    sns.heatmap(cm, annot=True, ax = ax, cmap='Blues', fmt=\"d\")\n\n    ax.set_title('Confusion Matrix')\n\n    ax.set_xlabel('Predicted Labels')\n    ax.set_ylabel('True Labels')\n\n    ax.xaxis.set_ticklabels(['FAKE', 'REAL'])\n    ax.yaxis.set_ticklabels(['FAKE', 'REAL'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_model = LSTM().to(device)\noptimizer = optim.Adam(best_model.parameters(), lr=0.001)\n\nload_checkpoint('/kaggle/working' + '/model.pt', best_model, optimizer)\n\nevaluate(best_model, test_iter)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}